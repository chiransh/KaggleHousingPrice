{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Team: ml_explorers\n\nTeam Members:\n1. Rishabh Bansal(rishabhb@usc.edu)\n2. Chiransh Gulati(cgulati@usc.edu)\n3. Ankur Kumar Goyal(akgoyal@usc.edu)","metadata":{}},{"cell_type":"markdown","source":"# Importing all packages","metadata":{}},{"cell_type":"code","source":"# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport warnings\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n\npd.set_option(\"display.max_rows\", 2000)\npd.set_option(\"display.max_columns\", 500)\npd.set_option(\"display.width\", 100)\npd.set_option(\"display.max_colwidth\", 500)\npd.set_option(\"display.float_format\", lambda x: \"%.2f\" % x)\n\nfrom sklearn.inspection import permutation_importance\nfrom IPython.display import display, HTML, display_html\n\ndisplay(HTML(\"<style>.container { width:100% !important; }</style>\"))\nfrom mlxtend.regressor import StackingCVRegressor\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.ticker as mticker\nfrom lightgbm import LGBMRegressor\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.compose import ColumnTransformer\nimport math\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.feature_selection import VarianceThreshold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# for variable transformation\nimport scipy.stats as stats\nfrom scipy.stats import norm, skew\n\nimport gc\ngc.enable()\nrandom_state = 123\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nsns.set(font_scale= 1)\npd.pandas.set_option('display.max_columns', None)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%%javascript\nIPython.OutputArea.auto_scroll_threshold = 9999# for variable transformation\nimport scipy.stats as stats\nfrom scipy.stats import norm, skew","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading Train and Test data sets.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A general overview of the data we need to work with. \nA few lines of code going from high level overview to lower/more detailed levels.","metadata":{}},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking to see data types and potential missing values**:","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Numerical/Categorical Features","metadata":{}},{"cell_type":"code","source":"# Identifying categorical_var variables\ncategorical_var = [var for var in train.columns if train[var].dtype=='O']\nprint('There are {} categorical_var variables'.format(len(categorical_var)))\n\n# Identifying numerical variables\nnumerical = [var for var in train.columns if train[var].dtype!='O']\nprint('There are {} numerical variables'.format(len(numerical)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Descriptive statistics:**","metadata":{}},{"cell_type":"code","source":"display(\n    train.describe().iloc[:, 0:18].applymap(\"{:,g}\".format),\n    train.describe().iloc[:, 18:].applymap(\"{:,g}\".format),\n)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Id = \"Id\"\nsubmission_ID = test.loc[:, Id]\ntrain.drop(Id, axis=1, inplace=True)\ntest.drop(Id, axis=1, inplace=True)\n\n# Creating Train flag to segregate train and test sets before concatinating\ntrain.loc[:, \"Train\"] = 1\ntest.loc[:, \"Train\"] = 0\ntest[\"SalePrice\"] = 0\ncombined_DF = pd.concat([train, test], ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Coorelation Analysis","metadata":{}},{"cell_type":"code","source":"corr_matrix =  train.corr()\nsns.set(rc = {'axes.facecolor':'white', 'figure.facecolor':'white'})\nf, ax  = plt.subplots(figsize= (12, 12))\nsns.heatmap(corr_matrix, vmax = .8, square=True, cmap=\"RdYlBu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####  Top 10 positively corelated features with 'SalePrice'","metadata":{}},{"cell_type":"code","source":"k = 10\ncols = corr_matrix.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nf , ax = plt.subplots(figsize = (12,12))\nhmm = sns.heatmap(cm,linewidths=0.01, annot=True, cmap = 'viridis', annot_kws={'size': 12}, square=True, xticklabels=cols.values, yticklabels = cols.values)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations\n1. 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'.\n2. 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However we know that the number of cars that fit into the garage is a consequence of the garage area.Therefore, we just need one of these variables in our analysis. We can keep 'GarageCars' since its correlation with 'SalePrice' is higher.","metadata":{}},{"cell_type":"markdown","source":"####  Top 10 negatively correlated features with 'SalePrice'","metadata":{}},{"cell_type":"code","source":"k = 10\ncols = corr_matrix.nlargest(1,'SalePrice')['SalePrice'].append(corr_matrix.nsmallest(k,'SalePrice')['SalePrice'])\ncols_index = cols.index\nprint(cols_index)\ncm = np.corrcoef(train[cols_index].values.T)\nf , ax = plt.subplots(figsize = (12,12))\nhmm = sns.heatmap(cm,linewidths=0.01, annot=True, cmap = 'viridis', annot_kws={'size': 12}, square=True,xticklabels=cols_index.values, yticklabels = cols_index.values)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attrs = corr_matrix.iloc[:-1,:-1] # all except target\nthreshold = 0.5\n\nimportant_corrs = (attrs[abs(attrs) > threshold][attrs != 1.0]).unstack().dropna().to_dict()\nunique_important_corrs = pd.DataFrame(list(set([(tuple(sorted(key)), important_corrs[key]) for key in important_corrs])), columns=['Feature_Pair', 'Corr'])\n\n# sorted by absolute value\nimp_corr = unique_important_corrs.iloc[abs(unique_important_corrs['Corr']).argsort()[::-1]]\nimp_corr\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Here we are trying to show the multicollinearity. In regression, multicollinearity refers to features that are coorelated with other features. Multicollonearity occurs when your model includes multiple factors that are coorrelated to each other as well and not just to your target variable.\n\nProblem:\nMulticollinearity increases the standard errors of the cooefficients. That means, multicollinearity makes some variables statistically insignificant when they should be significant.\n\nTo Avoid:\n1. Completely remove those variables\n2. Make new features by adding them or by some other operation.\n3. Use PCA, which will reduce feature set to small number of non-collinear feature","metadata":{}},{"cell_type":"code","source":"#coorelation with our target Variable - SalePrice\ncorr_matrix['SalePrice']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#####  As we saw there are few feature which shows high multicollinearity from heatmap.\n\nOur Main focus will be:\n\n1. SalePrice and OverallQual\n2. GarageArea and GarageCars\n3. TotalBsmtSF and 1stFlrSF\n4. GrLiveArea and TotRmsAbvGrd\n5. YearBulit and GarageYrBlt\n\n\nWe have to create a single feature or remove one of them before we use them as predictors.\n","metadata":{}},{"cell_type":"code","source":"num_feat = train.columns[train.dtypes != object]\nnum_feat = num_feat[1:-1]\nlabels = []\nvalues = []\nfor col in num_feat:\n    labels.append(col)\n    values.append(np.corrcoef(train[col].values, train.SalePrice.values)[0,1])\n    \n    \nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,40))\nrects = ax.barh(ind, np.array(values), color='green')\nax.set_yticks(ind+((width)/2.))\nax.set_yticklabels(labels, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation Coefficients w.r.t Sale Price\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations from Correlation plot\n- OverallQual, GrLivArea, GarageCars, GarageArea, TotalBsmtSF, 1stFlrSF, FullBath, TotRmsAbcGrd, YearBuilt, YearRemodAdd have positive correlation with SalePrice (> 0.5)\n- EnclosedPorch and KitchenAbvGr have negative correlation with target Variable\n- These features can prove to be important features to predict SalePrice.","metadata":{}},{"cell_type":"markdown","source":"####  Interesting Plot","metadata":{}},{"cell_type":"code","source":"train_scatter = train.copy()\ntrain_scatter = train_scatter.loc[train_scatter['TotalBsmtSF']<3500.0]\ntrain_scatter = train_scatter.loc[train_scatter['TotalBsmtSF']>0.0]\n\ndef plot_scatter(features,title):\n    fig, (axs,axs1) = plt.subplots(nrows=2, ncols=2, figsize=(20,13))\n    fig.subplots_adjust(wspace=0.5)\n    for i in range(0,len(features)):\n        plot = pd.concat([train_scatter['SalePrice'],train_scatter[features[i]]],axis = 1)\n        if i<2 :\n            g = sns.regplot(x=features[i],y = 'SalePrice',data = plot,scatter= True,\n                      truncate=True,x_estimator=np.mean,marker='.',\n                      fit_reg=True,ax=axs[i])\n        else:\n            g = sns.regplot(x=features[i],y = 'SalePrice',data = plot,scatter= True,\n                      truncate=True,x_estimator=np.mean,marker='.',\n                      fit_reg=True,ax=axs1[i-2])\n        fig.suptitle(\"Plotting highly \"+title+\"ly correlated variables with normalized SalePrice\",fontsize=\"15\")\n        if title=='negative':\n            g.set(xticklabels=feat_labels[features[i]])\n  \npos_features =['OverallQual','GrLivArea','GarageCars','TotalBsmtSF']\nplot_scatter(pos_features,'positive')\n\ndel train_scatter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target Variable Distribution - Univariate Analysis","metadata":{}},{"cell_type":"code","source":"# The density plot of SalePrice\nsns.distplot(train['SalePrice'], bins = 50)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From Here we can see that Sales Price is\n* Deviate from normal distribution\n* Is right-skewed  \n* shows some peakedness","metadata":{}},{"cell_type":"markdown","source":"# Target Variable Distribution - Continuous Bivariate Analyis","metadata":{}},{"cell_type":"code","source":"corr_matrix =  train.corr()\ncols = corr_matrix.nlargest(10, 'SalePrice')['SalePrice'].index\nsns.pairplot(train[cols], height=2.5)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####  Observations \n1. One of the intersting figure is 'TotalBsmtSF' and 'GrLiveArea'. We can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area\n2. The plot  'SalePrice' and 'YearBuilt' is also intersting. In the bottom of the 'dots cloud', we see what almost appears to be a shy exponential function. We can also see this same tendency in the upper limit of the 'dots cloud'. Also, we noticed how the set of dots regarding the last years tend to stay above this limit (I just wanted to say that prices are increasing faster now).\n","metadata":{}},{"cell_type":"markdown","source":"# Target Variable Distribution - Catagorical Bivariate Analyis","metadata":{}},{"cell_type":"code","source":"features_to_viz = [\"Neighborhood\", \"BsmtQual\", \"ExterQual\", \"FireplaceQu\", \"ExterCond\", \"KitchenQual\", \"LotShape\", \"OverallQual\", \"FullBath\", \"HalfBath\", \"TotRmsAbvGrd\", \"Fireplaces\", \"KitchenAbvGr\"]\nfeatures_to_viz = sorted(features_to_viz)\n\nncols = 1\nnrows = math.ceil(len(features_to_viz) / ncols)\nunused = nrows * ncols - len(features_to_viz)\n\n(figw, figh) = (ncols * 10, nrows * 8)\n\n(fig, ax) = plt.subplots(nrows, ncols, figsize=(figw, figh))\nfig.subplots_adjust(hspace=0.2, wspace=0.2)\n\nfor n,col in enumerate(features_to_viz):\n    ordering = combined_DF.loc[combined_DF[\"Train\"] == 1].groupby(by=col)[\"SalePrice\"].median().sort_values().index\n    sns.boxplot(x=\"SalePrice\", y=col, data=combined_DF.loc[combined_DF[\"Train\"] == 1], order=ordering, ax=ax[n], orient=\"h\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations\n\n - MasVnrType has strong correlation with 'YearBuilt' and 'OverallQual'which are already considered, so we can remove this variable\n - OverallQual impacts SalePrice exponentially\n - Features representing quality aspect of property like Neighbourhood matter a lot.\n - The more irregular the lot shape is, the higher the Sale Price seems to be","metadata":{}},{"cell_type":"markdown","source":"# Missing Values Analysis","metadata":{}},{"cell_type":"code","source":"print(\"Missing Value Counts in Training Data\")\ncombined_DF[combined_DF[\"Train\"] == 1].isna().sum()[combined_DF[combined_DF[\"Train\"] == 1].isna().sum()>0].sort_values(ascending=False)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Missing Values in Test Data\")\ncombined_DF[combined_DF[\"Train\"] == 0].isna().sum()[combined_DF[combined_DF[\"Train\"] == 0].isna().sum()>0].sort_values(ascending=False)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" #### Observations\n - We will mostly employ mass imputing strategy as opposed to deal with exceptions.\n - Most missing value indicate that particular house doesn't have that feature. Hence we can replace it with 'None'.","metadata":{}},{"cell_type":"markdown","source":"# Imputing Missing Values","metadata":{}},{"cell_type":"code","source":"# Due to relation between Neighborhood and MSZoning\nlookup = combined_DF.loc[combined_DF[\"Train\"] == 1].groupby(by=\"Neighborhood\")[\"MSZoning\"].agg(pd.Series.mode)\ncombined_DF[\"MSZoning\"] = combined_DF[\"MSZoning\"].fillna(combined_DF[\"Neighborhood\"].map(lookup))\n\n# Due to relation between KitchenQual and OverallQual\nlookup = combined_DF.loc[combined_DF[\"Train\"] == 1].groupby(by=\"OverallQual\")[\"KitchenQual\"].agg(pd.Series.mode)\ncombined_DF[\"KitchenQual\"] = combined_DF[\"KitchenQual\"].fillna(combined_DF[\"OverallQual\"].map(lookup))\n\n# Features where we replace nan values with a string indicator: \"None\"\ncols_na_to_none = {\"Alley\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFullBath\", \"BsmtQual\", \"Fence\", \"FireplaceQu\", \"GarageCond\", \"GarageFinish\", \"GarageQual\", \"GarageType\", \"MasVnrType\", \"MiscFeature\", \"PoolQC\", \"BsmtFinType2\"}\n\n# Features where we replace Nan values with integer 0\ncols_na_to_zero = {\"GarageArea\", \"GarageCars\", \"TotalBsmtSF\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtFullBath\", \"BsmtHalfBath\", \"GarageYrBlt\"}\n\n# Features where we replace Nan values with the mode of the feature values\ncols_na_to_mode = {\"Functional\", \"Electrical\", \"Utilities\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\"}\n\nfor col in cols_na_to_none: combined_DF[col] = combined_DF[col].astype(object).fillna(\"None\")\nfor col in cols_na_to_zero: combined_DF[col] = combined_DF[col].astype(object).fillna(0)\nfor col in cols_na_to_mode: combined_DF[col] = combined_DF[col].astype(object).fillna(combined_DF.loc[combined_DF[\"Train\"] == 1, col].mode()[0])\n\n# Imputing remaining missing values with the help of iterative imputer.\nnum_features = combined_DF.drop(columns=[\"Train\"]).select_dtypes(\"number\").columns\nimputer = IterativeImputer(RandomForestRegressor(max_depth=8), n_nearest_features=10, max_iter=10, random_state=random_state)\ncombined_DF.loc[combined_DF[\"Train\"] == 1, num_features] = imputer.fit_transform(combined_DF.loc[combined_DF[\"Train\"] == 1, num_features].values)\ncombined_DF.loc[combined_DF[\"Train\"] == 0, num_features] = imputer.transform(combined_DF.loc[combined_DF[\"Train\"] == 0, num_features].values)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering and Data Preprocessing\n\n#### Overview\n\n* Treating Outliers\n* Treating Missing Values in numerical and categorical features\n* we will remove skewenes of numeric features is exists\n* We will remove skewenes from target feature\n* And we will do feature selection\n\n#### Below we create hand engineered features via feature crossing and value mapping. These new features allow models to capture patterns that they wouldn't be able to capture from individual features.","metadata":{}},{"cell_type":"code","source":"# Extracting useful information from existing features to create new features\ncombined_DF[\"Season_Warm\"] = np.where(combined_DF[\"MoSold\"].isin([10, 11, 12, 1, 2, 3]), 0, 1)\ncombined_DF[\"SqFtAreaPerRoom\"] = combined_DF[\"GrLivArea\"] / (combined_DF[\"TotRmsAbvGrd\"]+combined_DF[\"FullBath\"]+combined_DF[\"HalfBath\"]+combined_DF[\"KitchenAbvGr\"])\n\n# Converting MSSubClass to categorical\ncombined_DF[\"MSSubClass\"] = combined_DF[\"MSSubClass\"].astype(str)\n\n\n# Ranking categorical features based on the median SalePrice they show for each category.\ncateogies_to_rank = [\"BsmtQual\", \"ExterQual\", \"ExterCond\", \"Exterior1st\", \"FireplaceQu\", \"GarageCond\", \"GarageQual\", \"Heating\", \"Fence\", \n                     \"HeatingQC\", \"OverallQual\", \"OverallCond\", \"HouseStyle\", \"KitchenQual\",\n                    \"LotShape\", \"BsmtCond\", \"MSSubClass\", \"Neighborhood\", \"SaleCondition\", \"SaleType\", \"MasVnrType\", \"ExterQual\"]\n\nfor col in cateogies_to_rank:\n    rank = np.array(range(0, len(combined_DF.loc[combined_DF[\"Train\"] == 1, col].unique())))\n    field_val = (combined_DF.loc[combined_DF[\"Train\"] == 1].groupby(by=col)[\"SalePrice\"].median().sort_values().index)\n    rankval_mapping = dict(zip(field_val, rank))\n    combined_DF[col + \"Rank\"] = combined_DF[col].map(rankval_mapping, na_action=\"ignore\")\n\n# Since there is an extra MSSubClass category in test set but not in train, we impute it with mode of the field to avoid nan values during ranking\ncombined_DF.loc[combined_DF[\"MSSubClassRank\"].isna(), \"MSSubClassRank\"] = 10\n\n\n# Combining underrepresented categories into one categorry and/or with another category in the same field\next2_map = {\"AsphShn\": \"Oth1\", \"CBlock\": \"Oth1\", \"CmentBd\": \"Oth2\", \"Other\": \"Oth2\"}\nroofmatl_map = {\"Roll\": \"Oth1\", \"ClyTile\": \"Oth1\", \"Metal\": \"Oth1\", \"CompShg\": \"Oth1\", \"Membran\": \"Oth2\", \"WdShake\": \"Oth2\"}\ncond2_map = {\"PosA\": \"Pos\", \"PosN\": \"Pos\", \"RRAe\": \"Norm\", \"RRAn\": \"Norm\"}\n\n\ncombined_DF[\"Exterior2nd\"] = combined_DF[\"Exterior2nd\"].map(ext2_map).fillna(combined_DF[\"Exterior2nd\"])\ncombined_DF[\"RoofMatl\"] = combined_DF[\"RoofMatl\"].map(roofmatl_map).fillna(combined_DF[\"RoofMatl\"])\ncombined_DF[\"Condition2\"] = combined_DF[\"Condition2\"].map(cond2_map).fillna(combined_DF[\"Condition2\"])\n\n# Creating new features via feature crossing (Intuition is based on trial and error)\ncombined_DF[\"Qual_Cond\"] = combined_DF[\"OverallQualRank\"] * combined_DF[\"OverallCondRank\"]\ncombined_DF[\"CombinedFlrSF\"] = combined_DF[\"1stFlrSF\"] + combined_DF[\"2ndFlrSF\"]\ncombined_DF[\"ExterHoodCond\"] = combined_DF[\"NeighborhoodRank\"] * combined_DF[\"ExterCondRank\"]\ncombined_DF[\"HoodFence\"] = combined_DF[\"NeighborhoodRank\"] * combined_DF[\"FenceRank\"]\ncombined_DF[\"AreaTotalCombined\"] = combined_DF[\"CombinedFlrSF\"] * combined_DF[\"OverallQualRank\"] * combined_DF[\"OverallCondRank\"]\ncombined_DF[\"KitchenQCCombinedFlrSF\"] = combined_DF[\"CombinedFlrSF\"] * combined_DF[\"KitchenQualRank\"]\ncombined_DF[\"HoodTotalQual\"] = combined_DF[\"NeighborhoodRank\"] * combined_DF[\"OverallQualRank\"]\ncombined_DF[\"MasVnrTypeHood\"] = combined_DF[\"NeighborhoodRank\"] * combined_DF[\"MasVnrTypeRank\"]\ncombined_DF[\"KitchenQualHood\"] = combined_DF[\"NeighborhoodRank\"] * combined_DF[\"KitchenQualRank\"]\ncombined_DF[\"Cond1Hood\"] = combined_DF[\"NeighborhoodRank\"] * combined_DF[\"Condition1\"]\ncombined_DF[\"Cond2Hood\"] = combined_DF[\"NeighborhoodRank\"] * combined_DF[\"Condition2\"]\ncombined_DF[\"PorchHood\"] = combined_DF[\"NeighborhoodRank\"] * combined_DF[\"3SsnPorch\"] + combined_DF[\"EnclosedPorch\"] + combined_DF[\"OpenPorchSF\"]\ncombined_DF[\"AgeSoldBuild\"] = combined_DF[\"YrSold\"] - combined_DF[\"YearBuilt\"]\ncombined_DF[\"AgeSoldRemod\"] = combined_DF[\"YrSold\"] - combined_DF[\"YearRemodAdd\"]\ncombined_DF[\"Age_Garage\"] = combined_DF[\"YrSold\"] - combined_DF[\"GarageYrBlt\"]\ncombined_DF[\"Remodeled\"] = combined_DF[\"YearBuilt\"] != combined_DF[\"YearRemodAdd\"]\ncombined_DF[\"AgeSoldBuild\"] = combined_DF[\"AgeSoldBuild\"].apply(lambda x: 0 if x < 0 else x)\ncombined_DF[\"AgeSoldRemod\"] = combined_DF[\"AgeSoldRemod\"].apply(lambda x: 0 if x < 0 else x)\ncombined_DF[\"Age_Garage\"] = combined_DF[\"Age_Garage\"].apply(lambda x: 0 if x < 0 else x)\n\nsqft_price_table = combined_DF.loc[combined_DF[\"Train\"] == 1].groupby(by=\"Neighborhood\")[\"SalePrice\", \"GrLivArea\"].agg(pd.Series.sum)\nsqft_price_table[\"AvgPrice\"] = sqft_price_table[\"SalePrice\"] / sqft_price_table[\"GrLivArea\"]\nsqft_price_table.drop(columns=[\"SalePrice\", \"GrLivArea\"], inplace=True)\ncombined_DF[\"AvgPricePerSqFtPerHood\"] = combined_DF[\"Neighborhood\"].map(sqft_price_table.to_dict()[\"AvgPrice\"])\n\n# Dropping features with high correlation with other variables \ncombined_DF.drop(columns=[\"GarageYrBlt\", \"Utilities\"], inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing correlation between new features and the target variable.\ncols_to_viz = [\"CombinedFlrSF\", \"KitchenQCCombinedFlrSF\", \"AreaTotalCombined\", \"HoodTotalQual\", \"MasVnrTypeHood\", \"KitchenQualHood\"]\n\nncols = 1\nnrows = math.ceil(len(cols_to_viz) / ncols)\nunused = (nrows * ncols) - len(cols_to_viz)\n\nfigw, figh = ncols*10, nrows*8\n\nfig, ax = plt.subplots(nrows, ncols, sharey=True, figsize=(figw, figh))\nfig.subplots_adjust(hspace=0.2, wspace=0.2)\nax = ax.flatten()\nfor i in range(unused, 0, -1):\n    fig.delaxes(ax[-i])\n\n\nfor n, col in enumerate(cols_to_viz):\n    if n % 2 != 0: ax[n].yaxis.label.set_visible(False)\n    ax[n].set_xlabel(col)\n    ax[n].set_ylabel(\"SalePrice\")\n    sns.scatterplot(x=col, y=\"SalePrice\", data=combined_DF.loc[combined_DF[\"Train\"] == 1], hue=\"SalePrice\", palette='gist_earth', s=75, legend=False, ax=ax[n])\nplt.show()\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dealing with highly skewed variables**","metadata":{}},{"cell_type":"code","source":"combined_DF[combined_DF[\"Train\"] == 1].skew()[abs(combined_DF[combined_DF[\"Train\"] == 1].skew()) > 5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we'll apply yeo-johnson transformation on highly skewed variables.\n\nhighly_skewed_cols = combined_DF[combined_DF[\"Train\"] == 1].skew()[abs(combined_DF[combined_DF[\"Train\"] == 1].skew()) > 5].index.to_list()\n\nptransformer = PowerTransformer(standardize=False)\ncombined_DF.loc[combined_DF[\"Train\"] == 1, highly_skewed_cols] = ptransformer.fit_transform(combined_DF.loc[combined_DF[\"Train\"] == 1, highly_skewed_cols])\ncombined_DF.loc[combined_DF[\"Train\"] == 0, highly_skewed_cols] = ptransformer.transform(combined_DF.loc[combined_DF[\"Train\"] == 0, highly_skewed_cols])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n - We'll apply one hot encoding to categorical variables, and scaling on numerical variables (except boolean-like features). \n - We'll then pass transformed variables through Variance Threshold to remove any feature with less variance.","metadata":{}},{"cell_type":"markdown","source":"Note: We apply seperate OneHot encoding to two different subset of categorical variables - test and train sets as few features have different set of categories.","metadata":{}},{"cell_type":"code","source":"# Obtaining a list of categorical, numerical, and boolean - like features.\nbool_features = [col for col in combined_DF.select_dtypes(include=[\"number\"]).columns if np.array_equal(np.sort(combined_DF[col].unique(), axis=0), np.sort([0, 1], axis=0))]\ncat_features = [col for col in combined_DF.select_dtypes(exclude=[\"number\"]).columns]\nnum_features = [col for col in combined_DF.select_dtypes(include=[\"number\"]).columns if col not in (bool_features) and col != \"SalePrice\"]\n\n# Keeping these two DFs to concatenate later with the preprocessed(scaled and onehot encoded) DF.\nbool_features.remove(\"Train\")\nX_train_bool = combined_DF.loc[combined_DF[\"Train\"] == 1, bool_features]\nX_test_bool = combined_DF.loc[combined_DF[\"Train\"] == 0, bool_features]\n\n# This list contains features that has the same set of values between train - test\nohe_cols_a = []\n\n# This list contains features that has different set of values between train - test\nohe_cols_b = []\n\nfor col in cat_features:\n    if set(combined_DF.loc[combined_DF[\"Train\"] == 1, col].unique()) != set(combined_DF.loc[combined_DF[\"Train\"] == 0, col].unique()): ohe_cols_b.append(col)\n\nohe_cols_a = list(set(cat_features) - set(ohe_cols_b))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = combined_DF.loc[combined_DF[\"Train\"] == 1].drop(labels=[\"SalePrice\", \"Train\"], axis=1)\n\n# Applying log transformation to the target variable\ny_train = combined_DF.loc[combined_DF[\"Train\"] == 1, \"SalePrice\"].apply(np.log)\nX_test = combined_DF.loc[combined_DF[\"Train\"] == 0].drop(labels=[\"SalePrice\", \"Train\"], axis=1)\n\npreprocessor = ColumnTransformer(transformers=[(\"onehota\", OneHotEncoder(sparse=False, drop=\"first\"), ohe_cols_a), (\"onehotb\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\"), ohe_cols_b), \n                                               (\"scaler\", StandardScaler(), num_features)],remainder=\"drop\")\n\n\npipeline = Pipeline([(\"Preprocessor\", preprocessor), (\"VarThreshold\", VarianceThreshold(0.01))])\nX_train_preprocessed = pipeline.fit_transform(X_train)\nX_test_preprocessed = pipeline.transform(X_test)\n\n# Get the list of one hot encoded columns and combine them\noh_encoded_a = list(preprocessor.named_transformers_.onehota.get_feature_names(ohe_cols_a))\noh_encoded_b = list(preprocessor.named_transformers_.onehotb.get_feature_names(ohe_cols_b))\noh_encoded_cols = oh_encoded_a + oh_encoded_b\nfeature_names = np.array(oh_encoded_cols + num_features, order=\"K\")\n\n# Filtering out the features dropped by variance threshold\nfeature_names = feature_names[pipeline.named_steps.VarThreshold.get_support()]\n\n# Putting back the column names to help with analysis\nX_train_preprocessed = pd.DataFrame(data=X_train_preprocessed, columns=feature_names)\nX_test_preprocessed = pd.DataFrame(data=X_test_preprocessed, columns=feature_names, index=X_test_bool.index)\n\n# Combine the DF's back together\nX_train = pd.concat([X_train_bool, X_train_preprocessed], axis=1)\nX_test = pd.concat([X_test_bool, X_test_preprocessed], axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importance","metadata":{}},{"cell_type":"code","source":"model = Lasso(alpha=0.01)\nmodel.fit(X_train, y_train)\n\n# Note: one pitfall of permutation importance is that it doesn't do the best job when it comes to correlated independent variables\nfeature_imp = permutation_importance(model, X_train, y_train, n_repeats=10, n_jobs=-1, random_state=random_state)\n\nperm_ft_imp_df = pd.DataFrame(data=feature_imp.importances_mean, columns=[\"FeatureImp\"], index=X_train.columns).sort_values(by=\"FeatureImp\", ascending=False)\nmodel_ft_imp_df = pd.DataFrame(data=model.coef_, columns=[\"FeatureImp\"], index=X_train.columns).sort_values(by=\"FeatureImp\", ascending=False)\n\nfig, ax = plt.subplots(2, 1, figsize=(12, 22))\n\nperm_ft_imp_df_nonzero = perm_ft_imp_df[perm_ft_imp_df[\"FeatureImp\"] != 0]\nmodel_ft_imp_df_nonzero = model_ft_imp_df[model_ft_imp_df[\"FeatureImp\"] != 0]\n\nsns.barplot(x=perm_ft_imp_df_nonzero[\"FeatureImp\"], y=perm_ft_imp_df_nonzero.index, ax=ax[0], palette=\"vlag\")\nsns.barplot(x=model_ft_imp_df_nonzero[\"FeatureImp\"], y=model_ft_imp_df_nonzero.index, ax=ax[1], palette=sns.diverging_palette(10, 220, sep=2, n=80))\nax[0].set_title(\"Permutation Feature Importance\")\nax[1].set_title(\"Lasso Feature Importance\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Features like OverallQual, Neighborhood, GrLivArea and Year built rank high in our models.\n- Some of the hand engineered features rank high in both lists.","metadata":{}},{"cell_type":"code","source":"rf_params = {\"max_depth\": 8, \"max_features\": 40, \"n_estimators\": 132}\nsvr_params = {\"kernel\": \"poly\",\"C\": 0.053677105521141605,\"epsilon\": 0.03925943476562099, \"coef0\": 0.9486751042886584}\nridge_params = {\"alpha\": 0.9999189637151178, \"tol\": 0.8668539399622242, \"solver\": \"cholesky\"}\nlasso_params = {\"alpha\": 0.0004342843645993161, \"selection\": \"random\"}\nlgbm_params = {\"num_leaves\": 16, \"max_depth\": 6, \"learning_rate\": 0.16060612646519587, \"n_estimators\": 64, \"min_child_weight\": 0.4453842422224686}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Comparison\n\nWe'll use cross-validate-score to compare model performance.","metadata":{}},{"cell_type":"code","source":"cv = KFold(n_splits=4, random_state=random_state,shuffle=True)\n\nsvr = SVR(**svr_params)\nridge = Ridge(**ridge_params, random_state=random_state)\nlasso = Lasso(**lasso_params, random_state=random_state)\nlgbm = LGBMRegressor(**lgbm_params, random_state=random_state)\nrf = RandomForestRegressor(**rf_params, random_state=random_state)\nstack = StackingCVRegressor(regressors=[svr, ridge, lasso, lgbm, rf], meta_regressor=LinearRegression(n_jobs=-1), random_state=random_state, cv=cv, n_jobs=-1)\n\nsvr_scores = cross_val_score(svr, X_train, y_train, cv=cv, n_jobs=-1, error_score='neg_root_mean_squared_error')\nridge_scores = cross_val_score(ridge, X_train, y_train, cv=cv, n_jobs=-1, error_score='neg_root_mean_squared_error')\nlasso_scores = cross_val_score(lasso, X_train, y_train, cv=cv, n_jobs=-1, error_score='neg_root_mean_squared_error')\nrf_scores = cross_val_score(rf, X_train, y_train, cv=cv, n_jobs=-1, error_score='neg_root_mean_squared_error')\nstack_scores = cross_val_score(stack, X_train, y_train, cv=cv, n_jobs=-1, error_score='neg_root_mean_squared_error')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = [svr_scores, ridge_scores, lasso_scores, rf_scores, stack_scores]\nmodels = [\"SVR\", \"RIDGE\", \"LASSO\", \"RF\", \"STACK\"]\nscore_medians = [round(np.median([mean for mean in modelscore]), 5) for modelscore in scores]\n\nfig, ax = plt.subplots(figsize=(14, 8))\nvertical_offset = 0.001\n\nax.set_title(\"Model Performance Comparison\")\nbp = sns.boxplot(x=models, y=scores, ax=ax)\n\nfor xtick in bp.get_xticks():\n    bp.text(xtick, score_medians[xtick] - vertical_offset, score_medians[xtick], horizontalalignment=\"center\", size=18, color=\"w\", weight=\"semibold\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n\nBelow we submit the inverse log-transformed results.<br><br>","metadata":{}},{"cell_type":"code","source":"svr.fit(X_train.values, y_train.values)\nstack.fit(X_train.values, y_train.values)\n\ndef blend_models_predict(X_test):\n    return ((0.6 * stack.predict(X_test.values)) + \\\n            (0.4 * svr.predict(X_test.values)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = blend_models_predict(X_test)\npredictions = np.exp(predictions)\n\nsubmission = pd.DataFrame({\"Id\": submission_ID, \"SalePrice\": predictions})\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}